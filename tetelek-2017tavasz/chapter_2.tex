%----------------------------------------------------------------------------
\chapter{Becslési módszerek}

\section{Maximum likelihood becslés}

A módszer alapgondolata, hogy ha egy kísérletnél több esemény is bekövetkezhet, akkor legtöbbször a legnagyobb valószínűségű esetményt fogjuk megfigyelni. Következtetésképp, azért kaptuk azt a mintát, amit, mert ennek a bekövetkezésnek volt a legnagyobb a valószínűsége. Tehát, az összes lehetséges $\vartheta$ paraméter közül vegyük azt, amelynél a kapott realizáció bekövetkezése a maximális! Képletesen: a paraméter maximum likelhood becslése az a $t_n(X_1,X_2, ..., X_n)$ statisztika, melyre $L(x,t_n(\mathbf{x})) = \text{max}_\vartheta L(\mathbf{x}, \vartheta)$ \footnote{$L(\mathbf{x}, \vartheta)$ a likelihood-függvény ($\sum_{i=1}^nP_\vartheta(X_i=x_i)$ diszkrét esetben és $\prod_{i=1}^nf_\vartheta(x_i)$ folytonos esetben.}.

A maximum meghatározásához $L(\mathbf{x}, \vartheta)$ $\vartheta$ szerinti első deriváltjának zérushelyeit keressük. A zérushelyek közül az a maximum, ahol a $L(\mathbf{x}, \vartheta)$ $\vartheta$ szerinti második deriváltja negatív. Megjegyzés: ha $L(\mathbf{x}, \vartheta)$ deriválása túl nehéz, érdemes a logaritmusát venni (log-likehood függvény, $l(\mathbf{x}, \vartheta)$) és azzal számolni. Mivel a logaritmus függvény szigorúan monoton nő, ezért nem torzítja a zérushelyeket. (Többparaméteres esetben paraméterenként vizsgáljuk az első deriváltakat és a Hesse-mátrix alapján döntünk a maximum helyről. Maximum esetén a mátrix diagonális és az első diagonál elem negatív.)

\emph{Cramer-Dugue-tétel:} Tegyük fel, hogy $t_n$ a $\vartheta$ paraméter maximum-likelihood becslése. Legyen a minta sűrűségfüggvénye $f_\vartheta(x)$, $\vartheta \in (a,b)$, ami kielégíti az alábbi feltételeket:
\begin{itemize}
\item létezik $ln \big(f_\vartheta(x) \big)$ első 3 deriváltja,
\item létezik $H_1(x), H_2(x), H_3(x)$, amelyek az indexnek megfelelő deriváltak abszolút értékeinek felső becslései, $H_1(x)$ és $H_2(x)$ teljes számegyenesen vett integráltjai léteznek, $H_3(x) \cdot f_\vartheta(x)$ teljes számegyenesen vett integrálja felülről korlátos és
\item $0<I_1(\vartheta)=\int_{- \infty}^{\infty} \big(\frac{\partial ln f_\vartheta(x)}{\partial \vartheta}\big)^2 \cdot f_\vartheta(x) dx < \infty$
\end{itemize}
Ekkor $t_n$ a $\vartheta$ paraméter konzisztens becslése és aszimptotikusan normális eloszlású. (Pluszban: ha létezik elégséges statisztika, akkor éppen azt adja meg, bár ez a tulajdonság nem tartozik a tételhez.)

\section{A momentumok módszere}

Legyen adott a valószínűségi mértékek egy tere és az $X_1, X_2, ..., X_n$ statisztikai minta. Tegyük fel, hogy létezik az első $k$ momentum ($m_j = \mathbf{E}_\mathbf{\vartheta}X_i^j$), és $\exists g_j^{-1}(m_1, m_2, ..., m_k) = \vartheta_j$. Tekintsük az $\hat{m}_j=\frac{1}{n}\sum_{i=1}^nX_i^j$ empirikus momentum statisztikákat. Ekkor az $m_j = g_j^{-1}(\hat{m}_1,..., \hat{m}_k)$ statisztikák a $\vartheta_j$ paraméterek momentumos becslései.

\section{Normális eloszlásból származtatott eloszlások}

\begin{table}[h]
\centering
\begin{tabular}{|p{2.7cm}|p{5.8cm}|p{5cm}|}
\hline
Név & Sűrűségfüggvény & Származtatása
\\ \hline
$\chi^2$-eloszlás & $ \frac{1}{2^{n/2}\Gamma(n/2)} \cdot e^{-x/2} \cdot x^{(n/2)-1}$ & \multirow{3}{5cm}{standard normális, független változók négyzetösszege $\chi^2_n$-eloszlást követ} \\
&  $ x>0 $ & \\
 & $\Gamma(s) = \int_0^\infty e^{-t} \cdot t^{s-1} dt$ &
\\ \hline
Student-eloszlás & $\frac{\Gamma(\frac{n+1}{2})}{\Gamma(1/2)\Gamma(n/2)} \cdot 1/\sqrt{n} \cdot \Big(\frac{1}{1+x^2/n} \Big)^{\frac{n+1}{2}}$ $x \in \mathbb{R}$ & standard normális, független valószínűségi változók esetén $\frac{Y}{\sqrt{\frac{\sum_{i=1}^{n}X_i^2}{n}}}$ $t_n$-eloszlást követ
\\ \hline
Fisher-eloszlás & $\frac{\Gamma(\frac{n+k}{2})}{\Gamma(n)\Gamma(k)} \cdot x^{(k/2)-1} \cdot \Big(k+nx \Big)^{-\frac{k+n}{2}}$ $x > 0$ & $X \in \chi^2_n$, $Y \in \chi^2_k$ és független $X$-től, akkor $\frac{X/n}{Y/k}$ $F_{n,k}$-eloszlást követ
\\ \hline
\end{tabular}
\end{table}

\emph{Lukács-tétel:} Legyenek $X_1, X_2,...X_n \in N(m,D)$ teljesen függetlenek. Ekkor
\begin{itemize}
\item az átlag $N(m,\frac{D}{\sqrt{n}})$ eloszlást követ
\item $\frac{ns_n^2}{D^2} \in \chi_{n-1}^2$
\item az átlag és $s_n^2$ függetlenek
\end{itemize} 

\section{Konfidencia intervallumok}

Eddig az ismeretlen paramétervektort a minta egy függvényével, azaz egyetlen statisztikával próbáltuk meg közelíteni. Konkrét realizációnál tehát, a paramétertér egy pontját egy másik ponttal becsüljük (\emph{pontbecslés}).

Folytonos eloszlásoknál azonban annak valószínősége, hogy a valószínőségi változó az értékkészletének éppen egy tetszőlegesen kiválasztott pontját fogja felvenni, nulla. Tehát folytonos esetben nulla annak valószínősége, hogy éppen a paramétert találtuk el a becsléssel. Az intervallumbecsléseknél a mintából készített tartományokat definiálunk, amely tartományok nagy valószínőséggel lefedik a kérdéses paraméterpontot.

Legyen adott a valószínűségi mértékek egy tere és az $X_1, X_2, ..., X_n$ statisztikai minta. Legyen $0<\epsilon<1$ rögzített. A $\vartheta$ paraméterhez megadhatunk egy legalább $1-\epsilon$ szignifikanciaszintű konfidencia-intervallumot, ha $t_1(X_1, X_2, ..., X_n)$ és $t_2(X_1, X_2, ..., X_n)$ olyan statisztikák, hogy:
$\mathbf{P}_\vartheta(t_1(X_1, X_2, ..., X_n) \leq \vartheta \leq t_2(X_1, X_2, ..., X_n)) \geq 1-\epsilon$ mindig igaz.

