%----------------------------------------------------------------------------
\chapter{Regresszióanalízis I.} \label{sec:reg1}

\section{Feltételes várható érték}

$\mathbf{E}(X|Y) = \int_{-\infty}^\infty x \cdot f_{X|Y}(x|y)dx=\frac{\int_{-\infty}^\infty x \cdot f_{X,Y}(x,y)dx}{f_Y(y)}$, amit regressziós görbének is nevezünk, ezzel lehet a legpontosabban közelíteni. Tulajdonságai:
\begin{itemize}
\item $\mathbf{E}(\mathbf{E}(X|Y)) = \mathbf{E}X$
\item $\mathbf{E}(h(Y) \cdot X | Y) = h(Y) \cdot \mathbf{E}(X|Y)$
\item Ha $X$ és $Y$ függetlenek, akkor $\mathbf{E}(X|Y) = \mathbf{E}X$
\item $\mathbf{E}(a \cdot X_1 + b \cdot X_2 |Y) = a \cdot \mathbf{E}(X_1|Y) + b \cdot \mathbf{E}(X_2|Y)$
\end{itemize}


\section{Regressziószámítás}

Regressziószámításkor egy változót egy vagy több másik változóval becslünk: a becsült változó a \emph{függőváltozó}, amivel becsüljük azt, az(ok) a \emph{független változó(k)}. A becslés annyit jelent, hogy egy $f$ függvénnyel szeretnénk leírni a függőváltozót, amely függvénynek a független változók az argumentumai és minimalizálja a négyzetes eltérés várható értékét. Ha ismernénk a függő és független változók együttes eloszlását, akkor probléma elméletileg megoldott, hiszek ekkor:

$f(X_1,...X_p) = \mathbf{E}(Y| X_1, ..., X_p)$

A gyakorlatban azonban csak egy adatmátrix adott és a függvénykapcsolatot a statisztikai minta alapján kell meghatározni. A regresszióanalízis végrehajtásának csak akkor van értelme, ha kimutatható a függő és független változók között az összefüggés (pl. el kellett vetni a nullhipotézist függetlenségvizsgálatnál).

Példák regresszióra:
\begin{itemize}

\item \emph{Lineáris regresszió:} $f(X) = B_0 + B_1X$ Mivel a gakorlatban nem ismertek a változók momentumai, ezért az egyes paramétereket becsülni kell. Erre van a legkisebb négyzetek módszere:
\begin{itemize}
\item \emph{Legkisebb négyzetek módszere:} Adott az $(X_1,Y_1), ... (X_p,Y_p)$ statisztikai minta és az $F = {f(x,a,b,c,...)}$ $k$-paraméteres függvényosztály. A $B_i$-ket a $\min\limits_{\forall a_1, ... a_k} \Sigma_{i=1}^p(Y_i - f(X_i; a_1, ..., a_k))^2$ szélsőérték feladat megoldásából kapjuk. Ez a módszer a legjobb tozítatlan becslést adja
\end{itemize}

A lineáris kapcsolat kitüntetett, mivel ez a legegyszerűbb és leggyakoribb, könnyű a két paramétert értelmezni, ráadásul kétdimenziós normális eloszlás esetén a kapcsolat nem is lehet más.

\item \emph{Polinomiális regresszió:} $f(X_1, ..., X_p) = B_0 + B_1X_1+B_2X_2+ ...+ B_pX_p$

\item \emph{Kétparaméteres (lineárisra visszavezethető) regresszió:} pl. $Y=f(X)=B_0\cdot e^{B_1X} \rightarrow lnY = B_1X + lnB_0$

\item \emph{Nem-lineáris regresszió:} elég sok fajta van, a lényeg, hogy $f$ nem csak lineáris összefüggéseket tartalmaz, hanem exponenciális, logaritmikus vagy éppen hányados tagokkal is rendelkezik. Néhány példa:
	\begin{itemize}
	\item Aszimptotikus 1: $f(x) = B_1 + B_2 e^{B_3X}$
	\item Aszimptotikus 2: $f(x) = B_1 + B_2 \cdot B_3^X$
	\item Sűrűség: $f(x) = (B_1 + B_2 X)^{-1/B_3}$
	\item Gauss: $f(x) = B_1 \cdot (1- B_3 e^{B_2X^2})$
	\item Gompertz: $f(x) = B_1 e^{ -B_2 e^{-B_3X^2}}$
	\item Johnson-Schumacher: $f(x) = B_1 e^{ -B_2 /(B_3 + X)}$
	\end{itemize}
\end{itemize}

A regressziót \emph{Naradaja módszerével} lehet közelíteni. A tökéletes függvénykapcsolatot a regressziós görbe adja meg. A sűrűségfüggvény becslését felhasználva a regressziós görbe konzisztens becslése:

$r_n(x) = \frac{\Sigma_{i=1}^n Y_i \cdot k\Big(\frac{x-X_i}{h_n}\Big)}{\Sigma_{i=1}^n k\Big(\frac{x-X_i}{h_n}\Big)}$, ahol
\begin{itemize}
\item $k(x)$ egy korlátos függvény, amelynek második momentuma véges és $|xk(x)| \rightarrow 0$ ha $|x| \rightarrow \infty$
\item $h_n>0$ egy számsorozat, hogy $h_n$ nullsorozat, $nh_n \rightarrow \infty$. A gyakorlatban sorozat helyett egy $h$ paraméterrel minimalizálunk
\end{itemize}

\section{A regresszió jósága}

A regresszióval számolt \emph{modell érvényességét} eldönthetjük szórásanalízissel. Ekkor a nullhipotézisünk, hogy a független változók mindegyike 0, vagyis egyik prediktor változó sem magyarázza a célváltozót. A nullhipotézisről F-próbával dönthetünk.

A \emph{meghatározottsági együttható} (R-squared) megmutatja, hogy a lineáris regresszióval a célváltozó mekkora hányadát lehet magyarázni. Értéke 0 és 1 között változik, számítása:

$R^2 = \frac{SSR = \Sigma_{i=1}^n\big(\tilde{Y}_i - \bar{Y}  \big)^2}{SSTO = \Sigma_{i=1}^n\big(Y_i - \bar{Y}  \big)^2}$

\emph{SSR:} Sum of Squares (regression), \emph{SSTO:} Sum of Squares (Total).
